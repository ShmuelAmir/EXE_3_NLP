> library(dplyr)
> library(stringr)
> library(quanteda)
> library(Matrix)
> library(caret)
> library(rpart)
> library(ggplot2)
> 
> # library(mosaic)
> # library(xtable)
> # library(gridExtra)
> # library(stopwords)
> 
> options(digits = 2)
> 
> # ---------- Load texts ----------#
> profiles <- read.csv('data/okcupid_profiles.csv.gz', header=TRUE, stringsAsFactors=FALSE)
> 
> 
> # ---------- Clean texts, tokenize, remove stop words ----------#
> profiles <- profiles %>% select(starts_with("essay"), sex)
> 
> essays <- select(profiles, starts_with("essay"))
> essays <- apply(essays, MARGIN = 1, FUN = paste, collapse=" ")
> 
> profiles_sex <- profiles$sex
> N <- length(profiles_sex)
> 
> rm(profiles)
> 
> html <- c( "<a[^>]+>", "class=[\"'][^\"']+[\"']", "&[a-z]+;", "\n", "\\n", "<br ?/>", "</[a-z]+ ?>" )
> stop.words <- c( "a", "am", "an", "and", "as", "at", "are", "be", "but", "can", "do", "for", "have", "i'm", "if", "in", "is", "it", "like", "love", "my", "of", "on", "or", "so", "that", "the", "to", "with", "you", "i" )
> 
> html.pat <- paste0( "(", paste(html, collapse = "|"), ")" )
> stop.words.pat <- paste0( "\\b(", paste(stop.words, collapse = "|"), ")\\b" )
> 
> essays <- str_replace_all(essays, html.pat, " ")
> essays <- str_replace_all(essays, stop.words.pat, " ")
> 
> rm(html, stop.words, html.pat, stop.words.pat)
> gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells 4.4e+06  237    1.2e+07  615  1.2e+07  615
Vcells 2.5e+07  194    9.4e+08 7176  1.2e+09 8970
> 
> all.tokens <- tokens(essays, what = "word",
+                      remove_numbers = TRUE, remove_punct = TRUE,
+                      remove_symbols = TRUE, remove_hyphens = TRUE)
Warning message:
remove_hyphens argument is not used. 
> all.tokens <- tokens_tolower(all.tokens)
> all.tokens <- tokens_select(all.tokens, stopwords(), selection = "remove")
> 
> 
> # ---------- Stemming ----------#
> all.tokens <- tokens_wordstem(all.tokens, language = "english")
> all.tokens <- tokens_select(all.tokens, "^[a-z]$", selection = "remove", valuetype = "regex")
> 
> 
> # ---------- DFM ----------#
> all.tokens.dfm <- dfm(all.tokens, tolower = FALSE)
> rm(essays, all.tokens)
> 
> dfm.trimmed <- dfm_trim(all.tokens.dfm, min_docfreq = 0.025 * N, min_termfreq = 0.025 * N, verbose = TRUE)
Removing features occurring: 
  - fewer than 1498 times: 157,551
  - in fewer than 1498 documents: 157,649
  Total features removed: 157,649 (99.3%).
> rm(all.tokens.dfm)
> gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells 4.4e+06  237    1.2e+07  615  1.2e+07  615
Vcells 2.1e+07  160    7.5e+08 5741  1.2e+09 8970
> 
> # ---------- TF - IDF ----------#
> 
> tokens.matrix <- convert(dfm.trimmed, to = "matrix")
> 
> tf <- prop.table(tokens.matrix, margin = 1)
> 
> idf <- apply(tokens.matrix, MARGIN = 2, function(x) log10(dim(tokens.matrix)[1] / sum(x != 0)))
> 
> train.dfm <- tf * idf
> 
> rm(tokens.matrix, tf, idf)
> gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells 4.4e+06  237    1.2e+07  615  1.2e+07  615
Vcells 8.8e+07  675    6.0e+08 4593  1.2e+09 8970
> 
> # ---------- male vs female by text ----------#
> 
> # df, names
> all.tokens.df <- as.data.frame(train.dfm, row.names = NULL, optional = FALSE, make.names = TRUE)
> 
> names(all.tokens.df) <- make.names(names(all.tokens.df), unique = TRUE) 
> 
> rm(dfm.trimmed)
> gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells 4.4e+06  237    1.2e+07  615  1.2e+07  615
Vcells 1.5e+08 1120    4.8e+08 3674  1.2e+09 8970
> 
> # cross validation
> cross_validation <- trainControl(method = "repeatedcv", number = 10, repeats = 3, verboseIter = TRUE)
> 
> # male, female labels
> all.tokens.df <- cbind(sex = profiles_sex, data = all.tokens.df)
> all.tokens.df <- na.omit(all.tokens.df)
> rm(profiles_sex)
> 
> # decision tree
> before <- Sys.time() # how to do for each repeat??????????
> train.model <- train(sex ~ ., data = all.tokens.df, trControl = cross_validation, method = "rpart")
+ Fold01.Rep1: cp=0.009288 
- Fold01.Rep1: cp=0.009288 
+ Fold02.Rep1: cp=0.009288 
- Fold02.Rep1: cp=0.009288 
+ Fold03.Rep1: cp=0.009288 
- Fold03.Rep1: cp=0.009288 
+ Fold04.Rep1: cp=0.009288 
- Fold04.Rep1: cp=0.009288 
+ Fold05.Rep1: cp=0.009288 
- Fold05.Rep1: cp=0.009288 
+ Fold06.Rep1: cp=0.009288 
- Fold06.Rep1: cp=0.009288 
+ Fold07.Rep1: cp=0.009288 
- Fold07.Rep1: cp=0.009288 
+ Fold08.Rep1: cp=0.009288 
- Fold08.Rep1: cp=0.009288 
+ Fold09.Rep1: cp=0.009288 
- Fold09.Rep1: cp=0.009288 
+ Fold10.Rep1: cp=0.009288 
- Fold10.Rep1: cp=0.009288 
+ Fold01.Rep2: cp=0.009288 
- Fold01.Rep2: cp=0.009288 
+ Fold02.Rep2: cp=0.009288 
- Fold02.Rep2: cp=0.009288 
+ Fold03.Rep2: cp=0.009288 
- Fold03.Rep2: cp=0.009288 
+ Fold04.Rep2: cp=0.009288 
- Fold04.Rep2: cp=0.009288 
+ Fold05.Rep2: cp=0.009288 
- Fold05.Rep2: cp=0.009288 
+ Fold06.Rep2: cp=0.009288 
- Fold06.Rep2: cp=0.009288 
+ Fold07.Rep2: cp=0.009288 
- Fold07.Rep2: cp=0.009288 
+ Fold08.Rep2: cp=0.009288 
- Fold08.Rep2: cp=0.009288 
+ Fold09.Rep2: cp=0.009288 
- Fold09.Rep2: cp=0.009288 
+ Fold10.Rep2: cp=0.009288 
- Fold10.Rep2: cp=0.009288 
+ Fold01.Rep3: cp=0.009288 
- Fold01.Rep3: cp=0.009288 
+ Fold02.Rep3: cp=0.009288 
- Fold02.Rep3: cp=0.009288 
+ Fold03.Rep3: cp=0.009288 
- Fold03.Rep3: cp=0.009288 
+ Fold04.Rep3: cp=0.009288 
- Fold04.Rep3: cp=0.009288 
+ Fold05.Rep3: cp=0.009288 
- Fold05.Rep3: cp=0.009288 
+ Fold06.Rep3: cp=0.009288 
- Fold06.Rep3: cp=0.009288 
+ Fold07.Rep3: cp=0.009288 
- Fold07.Rep3: cp=0.009288 
+ Fold08.Rep3: cp=0.009288 
- Fold08.Rep3: cp=0.009288 
+ Fold09.Rep3: cp=0.009288 
- Fold09.Rep3: cp=0.009288 
+ Fold10.Rep3: cp=0.009288 
- Fold10.Rep3: cp=0.009288 
Aggregating results
Selecting tuning parameters
Fitting cp = 0.00929 on full training set
> Sys.time() - before
Time difference of 13 hours
> 
> # confusion matrix
> confusionMatrix(train.model) #rpart
Cross-Validated (10 fold, repeated 3 times) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction  f  m
         f 16 12
         m 24 48
                           
 Accuracy (average) : 0.643

> 
> 
> # ---------- Remove batch effect ----------#
> male.df <- subset(all.tokens.df, sex == "m")
> female.df <- subset(all.tokens.df, sex == "f")
> 
> male_term_freq <- colSums(male.df[, -1])
> female_term_freq <- colSums(female.df[, -1])
> 
> num_most <- 25
> male_most_freq <- sort(male_term_freq, decreasing = TRUE)[1:num_most]
> female_most_freq <- sort(female_term_freq, decreasing = TRUE)[1:num_most]
> 
> male_most_freq
data.friend   data.work  data.music   data.good   data.time  data.thing  data.peopl     data.go   data.life   data.food 
        540         499         480         476         396         393         381         381         370         362 
  data.just   data.movi   data.make    data.get    data.new  data.enjoy   data.book   data.want   data.know   data.live 
        361         359         338         337         302         274         273         270         264         250 
  data.look data.realli    data.tri  data.think   data.also 
        250         247         241         235         223 
> female_most_freq
data.friend   data.good  data.music   data.work  data.peopl   data.time   data.life  data.thing   data.food     data.go 
        439         307         305         288         269         267         264         259         256         254 
  data.movi   data.make    data.new   data.just    data.get   data.book data.famili  data.enjoy   data.live   data.want 
        244         240         217         215         214         210         206         188         179         171 
data.realli   data.know    data.tri   data.also   data.look 
        171         167         161         161         154 
> 
> train.dfm <- train.dfm[, !colnames(train.dfm) %in% male_most_freq]
> train.dfm <- train.dfm[, !colnames(train.dfm) %in% female_most_freq]
> 
> train.dfm <- na.omit(train.dfm)
> 
> # ---------- Clustering ----------#
> 
> # Calculate PCA
> cluster.pca <- prcomp(train.dfm)
> pca.data_frame <- data.frame(Text = rownames(cluster.pca$x), x = cluster.pca$x[,1], y = cluster.pca$x[,2])
> pca.var <- cluster.pca$sdev ^ 2
> pca.var.per <- round(pca.var / sum(pca.var) * 100, 1)
> 
> colors <- paste0("cluster #", 1:10)
> 
> # plot PCA for each cluster result
> 
> pdf("Week5_datingNLP.pdf")
> 
> kmeans_plots <- function(k) {
+   clusters <- kmeans(train.dfm, k)
+   
+   ggplot(
+     data = pca.data_frame,
+     aes(x = x, y = y, label = Text, color = clusters$cluster)) +
+     geom_point() +
+     ggtitle(paste0("Kmeans: K = ", k, " clusters")) +
+     xlab(paste0("PC1 (", pca.var.per[1], "%)")) + 
+     ylab(paste0("PC2 (", pca.var.per[2], "%)")
+   )
+ }
> 
> kmeans_plots(2)
> kmeans_plots(3)
Warning message:
Quick-TRANSfer stage steps exceeded maximum (= 2886900) 
> kmeans_plots(4)
Warning message:
Quick-TRANSfer stage steps exceeded maximum (= 2886900) 
> kmeans_plots(10)
Warning message:
Quick-TRANSfer stage steps exceeded maximum (= 2886900) 
> 
> # plot T-SNE
> sign.pca <- pca.var.per[pca.var.per > 0.7]
> tsne.df <- data.frame(pc = seq(1:length(sign.pca)), val = sign.pca)
> 
> ggplot(data = tsne.df, aes(pc, val)) + 
+   geom_col()
> 
> save(file="Week5_datingNLP.rdata", train.model, train.dfm, cluster.pca)
> 
> dev.off()
null device 
          1 